{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ea8e3b-e941-4793-b863-030f82d17d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c27ed6b-39b7-4acc-9174-70cb755b3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102bd959-e483-4a25-9378-350b51ca4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separator(text):\n",
    "    return re.sub(pattern=\"<b.*/>|\\.|\\?|\\!\", string=text, repl=\" [SEP] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fda8ba-e4a9-4f53-b89f-acfc7e8b4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('files/book_user_14_20.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98817f7e-a739-44c1-9562-ab5ef7dd835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_count(a):\n",
    "    return a.overall.nunique()\n",
    "user_overall_nunique = df.groupby('reviewerID').apply(overall_count)\n",
    "uon_vc = pd.DataFrame(user_overall_nunique)[0].value_counts()\n",
    "overall_5_users = list(user_overall_nunique[user_overall_nunique==5].index)\n",
    "df = df[df.reviewerID.isin(overall_5_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bfad3cd-2224-45d7-a083-80a1c05a5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['reviewText','overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc56ffb-b44b-41e9-878a-9d198017a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'reviewText':'review','overall':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94832773-f1c5-40fe-a4c7-f9cd5d2cba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 693237 entries, 4 to 3647853\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   review     693153 non-null  object\n",
      " 1   sentiment  693237 non-null  int16 \n",
      "dtypes: int16(1), object(1)\n",
      "memory usage: 11.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aecdfb5-4ab6-40a3-994a-8ec5e9ee9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.sentiment==5)|(df.sentiment<3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee8f3e5-a985-4586-b8bf-d6f0a962a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~pd.isnull(df.review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4926cdb3-cfdc-4511-b588-25c751a77acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    306697\n",
       "2     51240\n",
       "1     41123\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d8645e-8b3a-4c20-a68f-1cef0b4855cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_random_ind = random.sample(list(df[df.sentiment==5].index), len(df[df.sentiment==5])//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "034be9a4-1235-4459-85d0-0b5b9b66e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df.loc[pos_random_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8970752-f7a5-457a-8921-55f604802c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ind = list(df[df.sentiment!=5].index)\n",
    "neg_df = df.loc[neg_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d34c0ae9-5626-41d5-bbfb-6b1095e65b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pos_df,neg_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1123417-a328-4dbd-9dc2-3a63486d23ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    102232\n",
       "2     51240\n",
       "1     41123\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b4530e0-9a40-49f7-b12c-f2920d1f8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.review\n",
    "classes = df.sentiment\n",
    "classes_oh = df.sentiment.apply(lambda x: 1 if x ==5 else 0)\n",
    "classes_oh = classes_oh.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf32bd4-eb99-4a79-953d-43b651a5208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(texts):\n",
    "    return [tokenizer.tokenize(\"[CLS] \" + text + \" [SEP]\") for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c321a1e-a65f-46b4-b1c2-26cfb359c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(t):\n",
    "    return tokenizer.tokenize(\"[CLS] \" + t + \" [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a8d8a7-3d87-475e-b5b0-d866952b6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa53c92-ea26-401d-abb5-ace3851d79a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194595/194595 [06:42<00:00, 483.21it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized = []\n",
    "for i in tqdm(texts):\n",
    "    tokenized.append(bert_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c80cb061-479b-49a5-8214-06b703732616",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99393244-eab0-4a17-a467-1ed2b31e8a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=194595, minmax=(3, 6735), mean=115.34015776356021, variance=34293.66305711972, skewness=5.284996801296655, kurtosis=63.82442108297337)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_tokens = np.array([len(bert_id) for bert_id in bert_ids])\n",
    "stats.describe(number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a4829cf-76b5-4e47-850c-3727d7305c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  7167,  1997, 14841, 18939, 12762,  2006,  6048,  1012,\n",
       "        2428,  5632,  2009,  1012,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 216\n",
    "padded_bert_ids = pad_sequences(bert_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n",
    "padded_bert_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b6587d-57ba-4289-9afe-c361a8aba7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194595/194595 [00:28<00:00, 6878.29it/s]\n"
     ]
    }
   ],
   "source": [
    "attention_masks = []\n",
    "for seq in tqdm(padded_bert_ids):\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3b2db1c-bfe9-4f10-85a4-709ad9431213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train: (122594, 216)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'X_val: (13622, 216)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'X_test: (58379, 216)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_train: (122594,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_val: (13622,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_test: (58379,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'masks_train: 122594'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'masks_val: 13622'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'masks_test: 58379'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_bert_ids, classes_oh, random_state=42, test_size=0.3)\n",
    "masks_train, masks_test, _, _ = train_test_split(attention_masks, padded_bert_ids, random_state=42, test_size=0.3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42, test_size=0.1)\n",
    "masks_train, masks_val, _, _ = train_test_split(masks_train, masks_train, random_state=42, test_size=0.1)\n",
    "\n",
    "display(\n",
    "    f\"X_train: {X_train.shape}\",\n",
    "    f\"X_val: {X_val.shape}\",\n",
    "    f\"X_test: {X_test.shape}\",\n",
    "    f\"y_train: {y_train.shape}\",\n",
    "    f\"y_val: {y_val.shape}\",\n",
    "    f\"y_test: {y_test.shape}\",\n",
    "    f\"masks_train: {len(masks_train)}\",\n",
    "    f\"masks_val: {len(masks_val)}\",\n",
    "    f\"masks_test: {len(masks_test)}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e03eab6d-a92a-4162-a1e0-df0cf5f90947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122594, 216])\n",
      "torch.Size([122594])\n",
      "torch.Size([122594, 216])\n",
      "torch.Size([13622, 216])\n",
      "torch.Size([13622])\n",
      "torch.Size([13622, 216])\n",
      "torch.Size([58379, 216])\n",
      "torch.Size([58379])\n",
      "torch.Size([58379, 216])\n"
     ]
    }
   ],
   "source": [
    "train_inputs = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "train_masks = torch.tensor(masks_train)\n",
    "validation_inputs = torch.tensor(X_val)\n",
    "validation_labels = torch.tensor(y_val)\n",
    "validation_masks = torch.tensor(masks_val)\n",
    "\n",
    "test_inputs = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "test_masks = torch.tensor(masks_test)\n",
    "\n",
    "print(train_inputs.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_masks.shape)\n",
    "print(validation_inputs.shape)\n",
    "print(validation_labels.shape)\n",
    "print(validation_masks.shape)\n",
    "print(test_inputs.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc13bfd-2bd4-4642-86e3-7205ebbeeefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22a7ef4b-2f06-466e-bfa2-141c26c20050",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "815ddd18-0caf-433e-870a-91c33ee9f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 3e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97be51b5-8ba3-4a44-abd0-8f369cddbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "413a37d0-f4c9-4d59-8be1-71ea8604bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d93e05fc-47d2-44b0-a078-c6733530f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "128860dc-fdb9-458e-adf0-a4ac99465c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래디언트 초기화\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c4c245e-7e0b-4809-8a8e-f40ca975ec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch   100  of  3,832.    Elapsed: 0:00:45.\n",
      "  Batch   200  of  3,832.    Elapsed: 0:01:25.\n",
      "  Batch   300  of  3,832.    Elapsed: 0:02:05.\n",
      "  Batch   400  of  3,832.    Elapsed: 0:02:45.\n",
      "  Batch   500  of  3,832.    Elapsed: 0:03:25.\n",
      "  Batch   600  of  3,832.    Elapsed: 0:04:04.\n",
      "  Batch   700  of  3,832.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  3,832.    Elapsed: 0:05:25.\n",
      "  Batch   900  of  3,832.    Elapsed: 0:06:05.\n",
      "  Batch 1,000  of  3,832.    Elapsed: 0:06:45.\n",
      "  Batch 1,100  of  3,832.    Elapsed: 0:07:25.\n",
      "  Batch 1,200  of  3,832.    Elapsed: 0:08:05.\n",
      "  Batch 1,300  of  3,832.    Elapsed: 0:08:45.\n",
      "  Batch 1,400  of  3,832.    Elapsed: 0:09:25.\n",
      "  Batch 1,500  of  3,832.    Elapsed: 0:10:05.\n",
      "  Batch 1,600  of  3,832.    Elapsed: 0:10:45.\n",
      "  Batch 1,700  of  3,832.    Elapsed: 0:11:25.\n",
      "  Batch 1,800  of  3,832.    Elapsed: 0:12:05.\n",
      "  Batch 1,900  of  3,832.    Elapsed: 0:12:45.\n",
      "  Batch 2,000  of  3,832.    Elapsed: 0:13:24.\n",
      "  Batch 2,100  of  3,832.    Elapsed: 0:14:04.\n",
      "  Batch 2,200  of  3,832.    Elapsed: 0:14:44.\n",
      "  Batch 2,300  of  3,832.    Elapsed: 0:15:24.\n",
      "  Batch 2,400  of  3,832.    Elapsed: 0:16:04.\n",
      "  Batch 2,500  of  3,832.    Elapsed: 0:16:43.\n",
      "  Batch 2,600  of  3,832.    Elapsed: 0:17:23.\n",
      "  Batch 2,700  of  3,832.    Elapsed: 0:18:03.\n",
      "  Batch 2,800  of  3,832.    Elapsed: 0:18:43.\n",
      "  Batch 2,900  of  3,832.    Elapsed: 0:19:23.\n",
      "  Batch 3,000  of  3,832.    Elapsed: 0:20:02.\n",
      "  Batch 3,100  of  3,832.    Elapsed: 0:20:42.\n",
      "  Batch 3,200  of  3,832.    Elapsed: 0:21:22.\n",
      "  Batch 3,300  of  3,832.    Elapsed: 0:22:02.\n",
      "  Batch 3,400  of  3,832.    Elapsed: 0:22:41.\n",
      "  Batch 3,500  of  3,832.    Elapsed: 0:23:21.\n",
      "  Batch 3,600  of  3,832.    Elapsed: 0:24:01.\n",
      "  Batch 3,700  of  3,832.    Elapsed: 0:24:41.\n",
      "  Batch 3,800  of  3,832.    Elapsed: 0:25:20.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:25:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch   100  of  3,832.    Elapsed: 0:00:40.\n",
      "  Batch   200  of  3,832.    Elapsed: 0:01:20.\n",
      "  Batch   300  of  3,832.    Elapsed: 0:02:00.\n",
      "  Batch   400  of  3,832.    Elapsed: 0:02:39.\n",
      "  Batch   500  of  3,832.    Elapsed: 0:03:19.\n",
      "  Batch   600  of  3,832.    Elapsed: 0:03:59.\n",
      "  Batch   700  of  3,832.    Elapsed: 0:04:39.\n",
      "  Batch   800  of  3,832.    Elapsed: 0:05:18.\n",
      "  Batch   900  of  3,832.    Elapsed: 0:05:58.\n",
      "  Batch 1,000  of  3,832.    Elapsed: 0:06:38.\n",
      "  Batch 1,100  of  3,832.    Elapsed: 0:07:18.\n",
      "  Batch 1,200  of  3,832.    Elapsed: 0:07:58.\n",
      "  Batch 1,300  of  3,832.    Elapsed: 0:08:37.\n",
      "  Batch 1,400  of  3,832.    Elapsed: 0:09:17.\n",
      "  Batch 1,500  of  3,832.    Elapsed: 0:09:57.\n",
      "  Batch 1,600  of  3,832.    Elapsed: 0:10:37.\n",
      "  Batch 1,700  of  3,832.    Elapsed: 0:11:17.\n",
      "  Batch 1,800  of  3,832.    Elapsed: 0:11:56.\n",
      "  Batch 1,900  of  3,832.    Elapsed: 0:12:36.\n",
      "  Batch 2,000  of  3,832.    Elapsed: 0:13:16.\n",
      "  Batch 2,100  of  3,832.    Elapsed: 0:13:56.\n",
      "  Batch 2,200  of  3,832.    Elapsed: 0:14:35.\n",
      "  Batch 2,300  of  3,832.    Elapsed: 0:15:15.\n",
      "  Batch 2,400  of  3,832.    Elapsed: 0:15:55.\n",
      "  Batch 2,500  of  3,832.    Elapsed: 0:16:34.\n",
      "  Batch 2,600  of  3,832.    Elapsed: 0:17:14.\n",
      "  Batch 2,700  of  3,832.    Elapsed: 0:17:54.\n",
      "  Batch 2,800  of  3,832.    Elapsed: 0:18:33.\n",
      "  Batch 2,900  of  3,832.    Elapsed: 0:19:13.\n",
      "  Batch 3,000  of  3,832.    Elapsed: 0:19:53.\n",
      "  Batch 3,100  of  3,832.    Elapsed: 0:20:32.\n",
      "  Batch 3,200  of  3,832.    Elapsed: 0:21:12.\n",
      "  Batch 3,300  of  3,832.    Elapsed: 0:21:51.\n",
      "  Batch 3,400  of  3,832.    Elapsed: 0:22:31.\n",
      "  Batch 3,500  of  3,832.    Elapsed: 0:23:11.\n",
      "  Batch 3,600  of  3,832.    Elapsed: 0:23:50.\n",
      "  Batch 3,700  of  3,832.    Elapsed: 0:24:30.\n",
      "  Batch 3,800  of  3,832.    Elapsed: 0:25:10.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:25:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch   100  of  3,832.    Elapsed: 0:00:40.\n",
      "  Batch   200  of  3,832.    Elapsed: 0:01:19.\n",
      "  Batch   300  of  3,832.    Elapsed: 0:01:59.\n",
      "  Batch   400  of  3,832.    Elapsed: 0:02:38.\n",
      "  Batch   500  of  3,832.    Elapsed: 0:03:18.\n",
      "  Batch   600  of  3,832.    Elapsed: 0:03:57.\n",
      "  Batch   700  of  3,832.    Elapsed: 0:04:37.\n",
      "  Batch   800  of  3,832.    Elapsed: 0:05:17.\n",
      "  Batch   900  of  3,832.    Elapsed: 0:05:56.\n",
      "  Batch 1,000  of  3,832.    Elapsed: 0:06:36.\n",
      "  Batch 1,100  of  3,832.    Elapsed: 0:07:16.\n",
      "  Batch 1,200  of  3,832.    Elapsed: 0:07:56.\n",
      "  Batch 1,300  of  3,832.    Elapsed: 0:08:35.\n",
      "  Batch 1,400  of  3,832.    Elapsed: 0:09:15.\n",
      "  Batch 1,500  of  3,832.    Elapsed: 0:09:55.\n",
      "  Batch 1,600  of  3,832.    Elapsed: 0:10:34.\n",
      "  Batch 1,700  of  3,832.    Elapsed: 0:11:14.\n",
      "  Batch 1,800  of  3,832.    Elapsed: 0:11:54.\n",
      "  Batch 1,900  of  3,832.    Elapsed: 0:12:34.\n",
      "  Batch 2,000  of  3,832.    Elapsed: 0:13:13.\n",
      "  Batch 2,100  of  3,832.    Elapsed: 0:13:53.\n",
      "  Batch 2,200  of  3,832.    Elapsed: 0:14:33.\n",
      "  Batch 2,300  of  3,832.    Elapsed: 0:15:12.\n",
      "  Batch 2,400  of  3,832.    Elapsed: 0:15:52.\n",
      "  Batch 2,500  of  3,832.    Elapsed: 0:16:32.\n",
      "  Batch 2,600  of  3,832.    Elapsed: 0:17:11.\n",
      "  Batch 2,700  of  3,832.    Elapsed: 0:17:51.\n",
      "  Batch 2,800  of  3,832.    Elapsed: 0:18:31.\n",
      "  Batch 2,900  of  3,832.    Elapsed: 0:19:11.\n",
      "  Batch 3,000  of  3,832.    Elapsed: 0:19:50.\n",
      "  Batch 3,100  of  3,832.    Elapsed: 0:20:30.\n",
      "  Batch 3,200  of  3,832.    Elapsed: 0:21:10.\n",
      "  Batch 3,300  of  3,832.    Elapsed: 0:21:50.\n",
      "  Batch 3,400  of  3,832.    Elapsed: 0:22:29.\n",
      "  Batch 3,500  of  3,832.    Elapsed: 0:23:09.\n",
      "  Batch 3,600  of  3,832.    Elapsed: 0:23:48.\n",
      "  Batch 3,700  of  3,832.    Elapsed: 0:24:28.\n",
      "  Batch 3,800  of  3,832.    Elapsed: 0:25:07.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:25:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # b_input_ids = b_input_ids.to(device)\n",
    "        # b_input_mask = b_input_mask.to(device)\n",
    "        # b_labels = b_labels.to(device)\n",
    "        # b_input_ids.to(device)\n",
    "        # b_input_mask.to(device)\n",
    "        # b_labels.to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ca57080-237e-4e59-8ed5-4f52528139d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,825.    Elapsed: 0:00:12.\n",
      "  Batch   200  of  1,825.    Elapsed: 0:00:25.\n",
      "  Batch   300  of  1,825.    Elapsed: 0:00:37.\n",
      "  Batch   400  of  1,825.    Elapsed: 0:00:50.\n",
      "  Batch   500  of  1,825.    Elapsed: 0:01:02.\n",
      "  Batch   600  of  1,825.    Elapsed: 0:01:14.\n",
      "  Batch   700  of  1,825.    Elapsed: 0:01:27.\n",
      "  Batch   800  of  1,825.    Elapsed: 0:01:39.\n",
      "  Batch   900  of  1,825.    Elapsed: 0:01:52.\n",
      "  Batch 1,000  of  1,825.    Elapsed: 0:02:04.\n",
      "  Batch 1,100  of  1,825.    Elapsed: 0:02:16.\n",
      "  Batch 1,200  of  1,825.    Elapsed: 0:02:29.\n",
      "  Batch 1,300  of  1,825.    Elapsed: 0:02:41.\n",
      "  Batch 1,400  of  1,825.    Elapsed: 0:02:54.\n",
      "  Batch 1,500  of  1,825.    Elapsed: 0:03:06.\n",
      "  Batch 1,600  of  1,825.    Elapsed: 0:03:18.\n",
      "  Batch 1,700  of  1,825.    Elapsed: 0:03:31.\n",
      "  Batch 1,800  of  1,825.    Elapsed: 0:03:43.\n",
      "\n",
      "Accuracy: 0.97\n",
      "Test took: 0:03:46\n"
     ]
    }
   ],
   "source": [
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "479dea12-8e8c-4c94-8605-394f6521aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 216\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9ebde1a2-c8e4-4615-87c2-f738926e2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f133140-cf87-488c-ac39-b1f47852e731",
   "metadata": {},
   "source": [
    "# 리뷰 긍부정 점수 컬럼 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a3116ef-b64e-48c6-9ba5-e339cbc4ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet('merge_book_2015_30_5.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "517e73a8-ab9f-44e7-b290-be006485f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_model = tf.saved_model.load('book_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "437a965e-8d3f-4f8c-917f-1ead5c2d670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = []\n",
    "for i in df2.reviewText:\n",
    "    if i != None:\n",
    "        df_reviews.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5da984a8-60ed-4caf-bfc3-76e6162b23d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "823236"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "33e61d53-f95d-4ca8-ab53-b15f8c262ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8233/8233 [1:27:22<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# 한번에 하면 메모리 에러 나서 100개씩 진행\n",
    "review_score_list = []\n",
    "for i in tqdm(range(len(df_reviews)//100+1)):\n",
    "    for j in test_sentences(df_reviews[i*100:i*100+100]):\n",
    "        review_score_list.append(np.argmax(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "92af46f9-149b-4a67-9c16-18da9cd87033",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_review_score_list = review_score_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cee4732c-8a8d-4fe0-9a8c-0ade659583bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review가 none 값 들어있는 index에 감성점수도 none값 넣어줌\n",
    "none_review_index = list(df2[pd.isnull(df2.reviewText)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "73c3181a-45b5-4abd-b393-1a4356680525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['review_sentiment_score'] = total_review_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e994304a-211c-4094-a00c-5d6baebea5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\n",
       "1    0.074405\n",
       "2    0.087934\n",
       "3    0.394422\n",
       "4    0.890668\n",
       "5    0.971910\n",
       "Name: review_sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('overall')['review_sentiment_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "320c9b8f-b4ef-4baa-8ba3-353e2394d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_parquet('torch_bert_merge_book_sentiment_2015_30_5.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1b7e3-f52d-4d1b-8495-dc71043adf83",
   "metadata": {},
   "source": [
    "### model 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "291da3f1-cb33-461c-991d-fe3eec9c98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"book_pytorch\", exist_ok=True)\n",
    "PATH = './book_pytorch/'\n",
    "torch.save(model, PATH + 'model.pt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1b673942-26ee-41d8-9b92-bfe08481272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load(PATH + 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6a858-c6dd-4c12-a3df-55d0368dc4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcc990-8cea-48b7-a7a9-bde9f1a8e006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
